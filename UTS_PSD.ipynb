{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Preprosesing"
      ],
      "metadata": {
        "id": "7oXzrtCcO8Yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install modul\n",
        "!pip install Sastrawi\n",
        "! pip install nltk"
      ],
      "metadata": {
        "id": "yd33poqBdYYA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8ccbdc2-6150-4973-fc4a-0a8531f3f1e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.7/209.7 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: Sastrawi\n",
            "Successfully installed Sastrawi-1.0.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.9/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk) (1.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk) (4.65.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import modul \n",
        "import pandas as pd\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "# create stemmer dan stopword removal\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "stop_factory = StopWordRemoverFactory()"
      ],
      "metadata": {
        "id": "Xezy2OUbfa1y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56fd0785-8033-4dee-f51c-65bdd4141fbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mendefinisikan variabel tanda baca dan stopword\n",
        "arrTdBaca = string.punctuation \n",
        "arrSwindo = stop_factory.get_stop_words()"
      ],
      "metadata": {
        "id": "kK1_M4eYg6NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mendefinisikan fungsi bukannum untuk mengecek apakah string adalah angka\n",
        "def bukannum(string):\n",
        "    pattern = r\"[^\\d]+\" # regex untuk mencocokkan karakter bukan angka\n",
        "    return re.match(pattern, string) is not None"
      ],
      "metadata": {
        "id": "5U8fvWdBYh6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#menghilangkan tag yang tidak diperlukan\n",
        "def cleansing_text(text):\n",
        "    cleaned_text = re.sub(\"<.*?>\", \"\", text)\n",
        "    return cleaned_text"
      ],
      "metadata": {
        "id": "wbhUtSKYiXjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# memproses teks dengan melakukan cleansing, tokenisasi, filtering, topword removal, case folding, and stemming \n",
        "def preprocessing(text):\n",
        "    # cleansing text\n",
        "    text = cleansing_text(text)\n",
        "\n",
        "    # tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # filtering punctuation and stopword\n",
        "    filtered_tokens = []\n",
        "    for token in tokens:\n",
        "\n",
        "        # filtering punctuation and stopword\n",
        "        if (token not in arrTdBaca) and (token not in arrSwindo) and bukannum(token):\n",
        "            filtered_tokens.append(token)\n",
        "\n",
        "    # case folding\n",
        "    filtered_tokens = [token.lower() for token in filtered_tokens]\n",
        "\n",
        "    # stemming\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    # join tokens into string\n",
        "    preprocessed_text = \" \".join(stemmed_tokens)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "# read csv file\n",
        "df = pd.read_csv('Data_TA.csv')\n",
        "\n",
        "# convert 'abstrak' column to string data type\n",
        "df['Abstrak'] = df['Abstrak'].astype(str)\n",
        "\n",
        "# preprocess text in the 'abstrak' column\n",
        "df['Abstrak'] = df['Abstrak'].apply(preprocessing)\n",
        "\n",
        "# save preprocessed data to new csv file\n",
        "df.to_csv('preprocessed_data.csv', index=False)"
      ],
      "metadata": {
        "id": "xBVFJetpiruA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Processing"
      ],
      "metadata": {
        "id": "pv9eC1S8PIz-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word2Vec"
      ],
      "metadata": {
        "id": "CURX3WrNPk-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLoHL_kNvT0M",
        "outputId": "18f3064a-e073-46be-f339-cb9ddf1bd0ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.9/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "\n",
        "# read preprocessed data\n",
        "df = pd.read_csv('preprocessed_data.csv')\n",
        "\n",
        "# handling missing value\n",
        "df['Abstrak'] = df['Abstrak'].fillna('')\n",
        "\n",
        "# label encoding\n",
        "label_encoder = LabelEncoder()\n",
        "df['Label_encoded'] = label_encoder.fit_transform(df['Label'])\n",
        "\n",
        "# tokenize text\n",
        "sentences = [sentence.split() for sentence in df['Abstrak']]\n",
        "\n",
        "# split dataset into training and testing data\n",
        "X_train, X_test, y_train, y_test = train_test_split(sentences, df['Label_encoded'], test_size=0.2)\n",
        "\n",
        "# Train the word2vec model\n",
        "w2v_model = gensim.models.Word2Vec(X_train,vector_size=100,window=5, min_count=2)\n",
        "\n",
        "# save trained model\n",
        "w2v_model.save('word2vec_cbow.model')\n",
        "\n",
        "#w2v_model.wv.index_to_key\n",
        "print(\"Shape of X_train:\", np.shape(X_train))\n",
        "print(\"Shape of X_test:\", np.shape(X_test))"
      ],
      "metadata": {
        "id": "U-MHjQTSrXAU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0efa3dfd-2408-4681-fcd1-44f4d21308aa"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_train: (682,)\n",
            "Shape of X_test: (171,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/numpy/core/fromnumeric.py:2007: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  result = asarray(a).shape\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer"
      ],
      "metadata": {
        "id": "XvsYHtE3QG6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "for i in range(len(X_train)):\n",
        "    X_train[i] = str(X_train[i])\n",
        "\n",
        "for i in range(len(X_test)):\n",
        "    X_test[i] = str(X_test[i])\n",
        "\n",
        "# create a CountVectorizer object\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "# fit and transform the training data\n",
        "X_train_vect = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# transform the testing data\n",
        "X_test_vect = vectorizer.transform(X_test)\n"
      ],
      "metadata": {
        "id": "fo43VW_i6_Me"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "Wx8TJg_OQeLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# train Naive Bayes model\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vect, y_train)\n",
        "\n",
        "# predict labels for testing data\n",
        "y_pred = nb.predict(X_test_vect)\n",
        "\n",
        "# evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# evaluate model\n",
        "#print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XeUBRp3FyCRg",
        "outputId": "a8b8f610-5a77-4b24-aecf-e28dc4335c4a"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8245614035087719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "Pd5y_YfRQhVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# train KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_vect, y_train)\n",
        "\n",
        "# predict labels for testing data\n",
        "y_pred = knn.predict(X_test_vect)\n",
        "\n",
        "# evaluate model\n",
        "#print(classification_report(y_test, y_pred))\n",
        "\n",
        "# evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A77XKhU53lMD",
        "outputId": "d6ac77a4-fafd-401b-a3d3-ffcf7584cfef"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.4152046783625731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF IDF"
      ],
      "metadata": {
        "id": "e_h1o5BJQ9E7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['Abstrak']\n",
        "y = df['Label']\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
        "countvectorizer = CountVectorizer()\n",
        "tfidfvectorizer = TfidfVectorizer()\n",
        "\n",
        "count_wm = countvectorizer.fit_transform(X)\n",
        "tfidf_wm = tfidfvectorizer.fit_transform(X)"
      ],
      "metadata": {
        "id": "K1UQwK9IFQva"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_wm.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kqY09JDcJ1WC",
        "outputId": "ede325a5-bbe5-4ba6-e536-6b24a7a702cf"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(853, 6781)"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA"
      ],
      "metadata": {
        "id": "DCyWQPF3QzMo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Impor library yang dibutuhkan\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Inisialisasi objek PCA dengan 3 komponen\n",
        "pca = PCA(n_components=3)\n",
        "\n",
        "for i in range(len(X)):\n",
        "    X[i] = str(X[i])\n",
        "\n",
        "# Melakukan fit transform pada data\n",
        "X_pca = pca.fit_transform(tfidf_wm.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jZGXNZ5J6zk",
        "outputId": "3f456208-e5e6-4cb9-ac91-a44db2b2d7e6"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-129-17572f382588>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X[i] = str(X[i])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_pca.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnxF09lzKD-C",
        "outputId": "1516b3f2-adb3-4d44-e9a0-d97756981442"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(853, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# split data into training and testing sets\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "XH7qgVb4KFy9"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# train Naive Bayes model\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_pca, y_train_pca)\n",
        "\n",
        "# predict labels for testing data\n",
        "y_pred = nb.predict(X_test)\n",
        "\n",
        "# evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "id": "2X2-hm3aKpcB",
        "outputId": "dd0f7c44-4bb2-446f-acb4-5fbbaf3168ab"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-146-1f2fef6b28c6>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# train Naive Bayes model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_pca\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_pca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# predict labels for testing data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_counters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 776\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    777\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_feature_log_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/naive_bayes.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m    896\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0;34m\"\"\"Count and smooth feature occurrences.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 898\u001b[0;31m         \u001b[0mcheck_non_negative\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MultinomialNB (input X)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    899\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_count_\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_non_negative\u001b[0;34m(X, whom)\u001b[0m\n\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mX_min\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1418\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Negative values in data passed to %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mwhom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Negative values in data passed to MultinomialNB (input X)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "\n",
        "# train KNN model\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train_pca, y_train_pca)\n",
        "\n",
        "# predict labels for testing data\n",
        "y_pred = knn.predict(X_test_pca)\n",
        "\n",
        "# evaluate model\n",
        "#print(classification_report(y_test, y_pred))\n",
        "\n",
        "# evaluate model\n",
        "accuracy = accuracy_score(y_test_pca, y_pred)\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_zW9S-tKSWQP",
        "outputId": "8439a162-ecaf-43f9-aaac-0b3d94d7d04b"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7283950617283951\n"
          ]
        }
      ]
    }
  ]
}